{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn\n",
    "import time\n",
    "from scipy.io import loadmat\n",
    "from numpy import fft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Definitions for ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkLayer:\n",
    "    def __init__(self, num_units, size_input=None, a_fcn ='sigmoid'):\n",
    "        self.num_units = num_units\n",
    "        self.size_input = size_input\n",
    "        self.layer_weights = None\n",
    "        self.a_fcn = a_fcn\n",
    "        self.h_a = None  \n",
    "        \n",
    "    def initialize_weights(self):\n",
    "        self.layer_weights = np.random.randn(self.num_units, self.size_input+1)\n",
    "        \n",
    "    def activate(self, net, a_fcn):\n",
    "        if a_fcn == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-net))\n",
    "        elif a_fcn == 'relu':\n",
    "            return np.maximum(0.001*net,net)\n",
    "        elif a_fcn == 'tanh':\n",
    "            return (np.exp(net)-np.exp(-net))/(np.exp(net)+np.exp(-net))\n",
    "        elif a_fcn == 'none':\n",
    "            return net\n",
    "        \n",
    "    def derivate(self, h_a, a_fcn):\n",
    "        if a_fcn == 'sigmoid':\n",
    "            return h_a * (1 - h_a)\n",
    "        elif a_fcn == 'relu':\n",
    "            return np.where(h_a < 0.0, 0.001*h_a, 1.0)\n",
    "        elif a_fcn == 'tanh':\n",
    "            return 1-h_a**2\n",
    "        elif a_fcn == 'none':\n",
    "            return 1 # NOT SURE\n",
    "     \n",
    "    def backprop_layer_error(self, delta, h_a):\n",
    "        derivative = self.derivate(h_a, self.a_fcn)\n",
    "        #print('derivative', derivative.shape)\n",
    "        #print('delta',delta.shape)\n",
    "        #print('weights',self.layer_weights.shape)\n",
    "        return np.matmul(delta,self.layer_weights)[:,1:] * derivative # trimmed to get rid of the bias term \n",
    "    \n",
    "    def __call__(self, X):\n",
    "        num_samples = X.shape[0]\n",
    "        layer_input = np.hstack([np.ones((num_samples,1)), X]) #add 1s for the bias term\n",
    "        net = np.matmul(layer_input, self.layer_weights.T)\n",
    "        \n",
    "        h_a = self.activate(net, self.a_fcn)\n",
    "        self.h_a = h_a #storing every hidden layer output\n",
    "        return h_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArtificialNeuralNetwork:\n",
    "    def __init__(self, network_layers, learning_rate):\n",
    "        self.learning_rate = learning_rate \n",
    "        layer_input_size = network_layers[0].num_units\n",
    "        network_layers[0].initialize_weights()\n",
    "         \n",
    "        for layer in network_layers[1:]:\n",
    "            layer.size_input = layer_input_size\n",
    "            layer_input_size = layer.num_units\n",
    "            layer.initialize_weights()\n",
    "            \n",
    "        self.network_layers = network_layers\n",
    "    \n",
    "    def forwardpropagate(self, X):\n",
    "        layer_output = self.network_layers[0](X)\n",
    "        for layer in self.network_layers[1:]:\n",
    "            layer_output = layer(layer_output)\n",
    "        return layer_output\n",
    "    \n",
    "    def bias(self,vec):\n",
    "        return np.hstack([np.ones((vec.shape[0], 1)), vec])\n",
    "    \n",
    "    def backpropagation(self, X, regression_output, true):\n",
    "        n_layers = len(self.network_layers)\n",
    "        delta = regression_output - true\n",
    "        n_a = regression_output\n",
    "        dWs = {}\n",
    "        \n",
    "        for i in range(-1,-len(self.network_layers),-1):\n",
    "            n_a = self.network_layers[i-1].h_a\n",
    "            \n",
    "            dWs[i] = np.matmul(delta.T,self.bias(n_a))\n",
    "            delta = self.network_layers[i].backprop_layer_error(delta,n_a)\n",
    "        \n",
    "        dWs[-n_layers] = np.matmul(delta.T,self.bias(X))\n",
    "        \n",
    "        for i_layer, dW in dWs.items():\n",
    "            self.network_layers[i_layer].layer_weights -= self.learning_rate * dW\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = np.load('./train_data_kspace.npz')\n",
    "k_train = training_data['train']\n",
    "#y_train = y_train[:,np.newaxis]\n",
    "test_data = np.load('./test_data_kspace.npz')\n",
    "k_test = test_data['test']\n",
    "#y_test = y_test[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_train_concat = ((k_train[1,:65536]+1j*k_train[1,65536:131072]))\n",
    "k_test_concat = ((k_test[3,:65536]+1j*k_test[3,65536:131072]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_k = np.reshape(k_test_concat,(256,256))\n",
    "sample_im = ifft2c(sample_k)\n",
    "plt.imshow(np.absolute(sample_im), cmap= 'gray')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_train_real = k_train[:,:65536]\n",
    "k_train_imag = k_train[:,65536:131072]\n",
    "k_test_real = k_test[:,:65536]\n",
    "k_test_imag = k_test[:,65536:131072]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = loadmat('ixi-t1_final.mat')['imdata']\n",
    "imdata = np.array(np.reshape(x,(581,256*256)))\n",
    "#imdata.shape\n",
    "im_train_true = imdata[:500,:]\n",
    "im_test_true = imdata[500:581,:]\n",
    "\n",
    "plt.imshow(np.absolute(x[1,:,:]), cmap= 'gray')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fft2c(im):\n",
    "    d = fft.fftshift(fft.fft2(fft.ifftshift(im)))\n",
    "    return np.real(d), np.imag(d)\n",
    "\n",
    "def ifft2c(d):\n",
    "    im = fft.fftshift(fft.ifft2(fft.ifftshift(d)))\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_train_true_real , k_train_true_imag = fft2c(im_train_true)\n",
    "#k_train_true = np.concatenate((k_train_true_real, k_train_true_imag), axis = 1)\n",
    "\n",
    "k_test_true_real, k_test_true_imag = fft2c(im_test_true)\n",
    "#k_test_true = np.concatenate((k_test_true_real, k_test_true_imag), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation function that seperates the training set into k folds and returns them\n",
    "def create_folds(k_train, k_true, num_folds):\n",
    "    permutation = np.random.permutation(k_train.shape[0])\n",
    "    k_train = k_train[permutation]\n",
    "    im_true = k_true[permutation]\n",
    "    \n",
    "    fold_size = int(k_train.shape[0]/num_folds)\n",
    "    k_train_cv = np.zeros((num_folds,fold_size,k_train.shape[1]))\n",
    "    #print(X_cv.shape)\n",
    "    k_true_cv = np.zeros((num_folds,fold_size,k_true.shape[1]))\n",
    "    #print(y_cv.shape)\n",
    "    \n",
    "    #i_sample = 0\n",
    "    for i_fold in range(num_folds):\n",
    "        for i_sample in range(fold_size):\n",
    "            sample_index = i_fold*fold_size + i_sample\n",
    "            #print(sample_index)\n",
    "            k_train_cv[i_fold,i_sample,:] = k_train[sample_index,:]\n",
    "            k_true_cv[i_fold,i_sample,:] = k_true[sample_index,:]\n",
    "        \n",
    "        print('Average of fold %d is %f' %(i_fold, np.average(k_train_cv[i_fold,:,:])))\n",
    "        print('Std of fold %d is %f' %(i_fold, np.std(k_train_cv[i_fold,:,:])))\n",
    "    return k_train_cv,k_true_cv\n",
    "\n",
    "k_train_true_real_cv, k_train_true_imag_cv = create_folds(k_train_true_real,k_train_true_imag,10)\n",
    "k_test_true_real_cv, k_test_true_imag_cv = create_folds(k_test_true_real,k_test_true_imag,10)\n",
    "k_train_real_cv, k_train_imag_cv = create_folds(k_train_real, k_train_imag,10)\n",
    "k_test_real_cv, k_test_imag_cv = create_folds(k_test_real, k_test_imag,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = k_train_real.shape[1]\n",
    "model_real = ArtificialNeuralNetwork([\n",
    "    NetworkLayer(8, a_fcn='sigmoid', size_input=N),\n",
    "    NetworkLayer(16, a_fcn='sigmoid'),\n",
    "    NetworkLayer(N, a_fcn='none'),\n",
    "],learning_rate = 0.001)\n",
    "\n",
    "N = k_train_imag.shape[1]\n",
    "model_imag = ArtificialNeuralNetwork([\n",
    "    NetworkLayer(8, a_fcn='sigmoid', size_input=N),\n",
    "    NetworkLayer(16, a_fcn='sigmoid'),\n",
    "    NetworkLayer(N, a_fcn='none'),\n",
    "],learning_rate = 0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_folds = 10\n",
    "for e in range(10):\n",
    "    print('============EPOCH %d ============' %(e))\n",
    "    for i_fold in range(num_folds):\n",
    "        print('**********FOLD %d **********' %(i_fold))\n",
    "        k_cv_test = k_train_real_cv[i_fold,:,:]\n",
    "        k_cv_test_true = k_train_true_real_cv[i_fold,:]\n",
    "        k_cv_train = np.concatenate((k_train_real_cv[:i_fold,:,:],k_train_real_cv[i_fold+1:num_folds,:,:]))\n",
    "        k_cv_train_true = np.concatenate((k_train_true_real_cv[:i_fold,:],k_train_true_real_cv[i_fold+1:num_folds,:]))\n",
    "        \n",
    "        reg_out_real = model_real.forwardpropagate(k_cv_train)\n",
    "        model_real.backpropagation(k_cv_train, reg_out_real, k_cv_train_true)\n",
    "        \n",
    "        print('Training with fold %d' %(i_fold))\n",
    "        print('Training Error (SSE) %f' %(np.sum((k_cv_train_true-reg_out_real)**2)/k_cv_train_true.shape[0]))\n",
    "        print('Testing with fold %d' %(i_fold))\n",
    "        \n",
    "        reg_out_real = model_real.forwardpropagate(k_cv_test)\n",
    "        model_real.backpropagation(k_cv_fest, reg_out_real, k_cv_test_true)\n",
    "        print('Test Error (SSE) %f' %(np.sum((k_cv_test-reg_out_real)**2)/k_cv_test.shape[0]))\n",
    "        \n",
    "        k_cv_test = k_train_imag_cv[i_fold,:,:]\n",
    "        k_cv_test_true = k_train_true_imag_cv[i_fold,:]\n",
    "        k_cv_train = np.concatenate((k_train_imag_cv[:i_fold,:,:],k_train_imag_cv[i_fold+1:num_folds,:,:]))\n",
    "        k_cv_train_true = np.concatenate((k_train_true_imag_cv[:i_fold,:],k_train_true_imag_cv[i_fold+1:num_folds,:]))\n",
    "        \n",
    "        reg_out_imag = model_imag.forwardpropagate(k_cv_train)\n",
    "        model_imag.backpropagation(k_cv_train, reg_out_imag, k_cv_train_true)\n",
    "        print('Training with fold %d' %(i_fold))\n",
    "        print('Training Error (SSE) %f' %(np.sum((k_cv_train_true-reg_out_imag)**2)/k_cv_train_true.shape[0]))\n",
    "        print('Testing with fold %d' %(i_fold))\n",
    "        \n",
    "        reg_out_imag = model_imag.forwardpropagate(k_cv_test)\n",
    "        model_imag.backpropagation(k_cv_fest, reg_out_imag, k_cv_test_true)\n",
    "        print('Test Error (SSE) %f' %(np.sum((k_cv_test-reg_out_imag)**2)/k_cv_test.shape[0]))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Normally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "for e in range(100):\n",
    "    reg_out_real = model_real.forwardpropagate(k_train_real)\n",
    "    model_real.backpropagation(k_train_real, reg_out_real, k_train_true_real)\n",
    "    if e % 100 == 0:\n",
    "        print(e)\n",
    "print('MLP trains in %s seconds' %(time.time() - start_time))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "for e in range(100):\n",
    "    reg_out_imag = model_imag.forwardpropagate(k_train_imag)\n",
    "    model_imag.backpropagation(k_train_imag, reg_out_imag, k_train_true_imag)\n",
    "    if e % 100 == 0:\n",
    "        print(e)\n",
    "print('MLP trains in %s seconds' %(time.time() - start_time))        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
